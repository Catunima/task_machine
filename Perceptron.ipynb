{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron\n",
        "The perceptron is like a basic decision-making unit in the world of machine learning. Imagine you have a black box that can take a set of inputs, like numbers, and make a decision, such as \"Yes\" or \"No.\" This black box takes the inputs, multiplies them by some values (weights), and adds them up. If the resulting sum is greater than a certain threshold (called bias), the black box says \"Yes\"; otherwise, it says \"No.\"\n",
        "\n",
        "In more concrete terms, the perceptron is used to solve binary classification problems, like deciding if an email is spam or not, or if a student will pass or fail an exam based on their study hours. The perceptron learns to adjust the weights and bias so that it can make correct decisions based on the training data. It's a simplified way to model how neurons in the human brain can make simple decisions."
      ],
      "metadata": {
        "id": "CKJ8q8Nao2bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PseudoCode"
      ],
      "metadata": {
        "id": "qaqvYdP_pRY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Initialize weights (w) and bias (b) randomly or to zero.\n",
        "Set the learning rate (learning_rate) and the maximum number of epochs (max_epochs).\n",
        "\n",
        "For each epoch in [1, 2, ..., max_epochs]:\n",
        "    For each training example (X, y):\n",
        "        Calculate the perceptron's output:\n",
        "            z = (X * w) + b\n",
        "            a = activation_function(z)  # Typically, the activation function is the step function.\n",
        "\n",
        "        Calculate the error (error):\n",
        "            error = y - a\n",
        "\n",
        "        Update the weights and bias:\n",
        "            w = w + learning_rate * error * X\n",
        "            b = b + learning_rate * error\n",
        "\n",
        "End of the epoch loop.\n",
        "\n",
        "The perceptron has been trained.\n",
        "\n",
        "To make predictions on new data:\n",
        "    Given a new input example (X_new):\n",
        "        Calculate the perceptron's output for X_new:\n",
        "            z = (X_new * w) + b\n",
        "            a = activation_function(z)\n",
        "\n",
        "        The final prediction is a.\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "s8e-BEqipTtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation"
      ],
      "metadata": {
        "id": "7T0SZ1M2o5eX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXsJDodwlH0W"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, epochs=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = np.zeros(2)  # Dos características: horas de estudio y sesgo\n",
        "\n",
        "    def predict(self, hours_studied):\n",
        "        summation = np.dot(self.weights, [1, hours_studied])\n",
        "        return 1 if summation > 0 else 0\n",
        "\n",
        "    def train(self, training_data, labels):\n",
        "        for _ in range(self.epochs):\n",
        "            for hours_studied, label in zip(training_data, labels):\n",
        "                prediction = self.predict(hours_studied)\n",
        "                error = label - prediction\n",
        "\n",
        "                # Actualizar los pesos con el optimizador (SGD)\n",
        "                self.weights += self.learning_rate * error * np.array([1, hours_studied])\n"
      ],
      "metadata": {
        "id": "0LqLPDkqnyOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Datos de entrenamiento (horas de estudio) y etiquetas (0=reprobado, 1=aprobado)\n",
        "    training_data = np.array([1, 2, 3, 4, 5])\n",
        "    labels = np.array([0, 0, 0, 1, 1])\n",
        "\n",
        "    # Crear y entrenar el perceptrón\n",
        "    perceptron = Perceptron()\n",
        "\n",
        "    for epoch in range(100):\n",
        "        total_loss = 0\n",
        "        for hours_studied, label in zip(training_data, labels):\n",
        "            prediction = perceptron.predict(hours_studied)\n",
        "            error = label - prediction\n",
        "\n",
        "            # Calcular la pérdida (MSE) y actualizar los pesos\n",
        "            loss = 0.5 * error**2\n",
        "            total_loss += loss\n",
        "            perceptron.weights += perceptron.learning_rate * error * np.array([1, hours_studied])\n",
        "\n",
        "    print(f\"Total Loss: {total_loss}\")\n",
        "\n",
        "    # Hacer predicciones\n",
        "    test_hours = [2.5, 3.5, 4.5]\n",
        "    for hours in test_hours:\n",
        "        prediction = perceptron.predict(hours)\n",
        "        result = \"Aprobado\" if prediction == 1 else \"Reprobado\"\n",
        "        print(f\"Estudiando {hours} horas: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM5iqd5mn1tj",
        "outputId": "88c924c4-77e0-4324-a921-331691816569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Loss: 0.0\n",
            "Estudiando 2.5 horas: Reprobado\n",
            "Estudiando 3.5 horas: Reprobado\n",
            "Estudiando 4.5 horas: Aprobado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loss and optimitation function\n",
        "###Loss Function:\n",
        "\n",
        "The loss function measures the error or the difference between the predicted values generated by the model and the actual ground truth (target) values in the training data.\n",
        "It quantifies how well or poorly the model is performing. The goal is to minimize this error during training.\n",
        "The choice of the loss function depends on the specific problem and the type of task. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy (Log Loss) for classification tasks.\n",
        "###Optimization Function (Optimizer):\n",
        "\n",
        "The optimization function, often referred to as the optimizer, is responsible for adjusting the model's parameters (weights and biases) to minimize the loss function.\n",
        "It uses optimization algorithms to update the model's parameters iteratively in the direction that reduces the loss. Common optimizers include Stochastic Gradient Descent (SGD), Adam, and RMSprop.\n",
        "The optimizer controls how quickly the model learns (learning rate), the size of the steps taken during parameter updates, and the convergence properties."
      ],
      "metadata": {
        "id": "GmpuSMR0se6l"
      }
    }
  ]
}